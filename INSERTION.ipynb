{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "from itertools import islice\n",
    "from heapq import nlargest\n",
    "from shared import *          #shared functions from shared.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-gram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_bi_gram():\n",
    "    \"\"\"store bi-gram as a list of lists of three elements: word, \n",
    "    the following word and probability of occurance of the following word\"\"\"\n",
    "    \n",
    "    bi_gram=[]\n",
    "    with open('../lm_unpruned', 'r') as f:\n",
    "        for line in islice(f, 57292, 1380576, 1): #read in only bi-grams \n",
    "            if line.split(' ')[1] not in ('<unk>', '</s>') and '_' not in line.split(' ')[1] and line.split(' ')[2].replace(\"\\n\", '') not in ('<unk>', '</s>') and '_' not in line.split(' ')[2].replace(\"\\n\", ''):\n",
    "                bi_gram.append([float(line.split(' ')[0]), line.split(' ')[1], line.split(' ')[2].replace(\"\\n\", '')])\n",
    "    return bi_gram\n",
    "\n",
    "bi_gram_list = read_bi_gram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bi_gram_dict(bi_gram_list):\n",
    "    \"\"\"create bi-gram dictionary based on the list of bi-grams\"\"\"\n",
    "    \n",
    "    bi_gram_dict = {}\n",
    "    for i in range(len(bi_gram_list)): \n",
    "        if bi_gram_list[i][1] in bi_gram_dict.keys():  \n",
    "            bi_gram_dict[bi_gram_list[i][1]].append([bi_gram_list[i][0], bi_gram_list[i][2]])  \n",
    "        else:  \n",
    "            bi_gram_dict[bi_gram_list[i][1]] = [[bi_gram_list[i][0], bi_gram_list[i][2]]]    \n",
    "    return bi_gram_dict   \n",
    "\n",
    "bi_gram_dict = create_bi_gram_dict(bi_gram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insertion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_words(transcript, rate):\n",
    "    \"\"\"insert words into the transcript at a given rate from bi-gram dictionary (uni-gram list if word is not in bi-gram), \n",
    "    return the new transcript and a list of inserted words\"\"\"\n",
    "    \n",
    "    transcript = json.loads(transcript)\n",
    "    to_insert = random_words_list(flatten(transcript), rate)\n",
    "    inserted_words = []        \n",
    "    try:\n",
    "        while 0 != (len(to_insert)):\n",
    "            \n",
    "            for sublist in transcript: \n",
    "                for element in sublist['tokens']:                \n",
    "                    if element['type'] not in('REF', 'INS', 'INS_SEC', 'RND'):      #avoid manipulating words that were already altered i.e. inserted or inserted after                     \n",
    "                        if to_insert[0] == element['value']:                             \n",
    "                            to_insert.remove(to_insert[0])                  #remove word from the list, the next element becomes index 0 and will be looked at once this loop is complete              \n",
    "                            if element['value'].lower() in bi_gram_dict:\n",
    "                                if len(bi_gram_dict[element['value'].lower()]) > 1:         #check that bi-gram key has more than one value    \n",
    "                                    first_max, second_max = nlargest(2, bi_gram_dict[element['value'].lower()])                #store two words fist_max/second_max that are more likely to occure according to bi-gram dictionary\n",
    "                                    if sublist['tokens'].index(element) == len(sublist['tokens'])-1:            #if the word after wich we need to insert is the last in the token, insert first_max                                                                            \n",
    "                                        sublist['tokens'].insert(sublist['tokens'].index(element)+1, {'type': 'INS', 'value': first_max[1]})                  #change 'type' to INS so the word is not used as a reference for insertion in future loops\n",
    "                                        element['type'] = 'REF'             #change 'type' of the word that was used as a reference for insertion so not to use it for other insertions\n",
    "                                        inserted_words.append({'type': 'word', 'value': first_max[1]})                                           \n",
    "                                    else:\n",
    "                                        if first_max[1] != sublist['tokens'][sublist['tokens'].index(element)+1]['value'].lower():      #check if the first_max from bi-gram is the same as the word following the word that we use as a reference for insertion                                          \n",
    "                                            sublist['tokens'].insert(sublist['tokens'].index(element)+1, {'type': 'INS', 'value': first_max[1]})\n",
    "                                            element['type'] = 'REF'   \n",
    "                                            inserted_words.append({'type': 'word', 'value': first_max[1]})  \n",
    "                                            \n",
    "                                        else:                                                                  #insert second_max, second most probable word from bi-gram dict                                                                                  \n",
    "                                            sublist['tokens'].insert(sublist['tokens'].index(element)+1, {'type': 'INS_SEC', 'value': second_max[1]})\n",
    "                                            element['type'] = 'REF'\n",
    "                                            inserted_words.append({'type': 'word', 'value': second_max[1]})                                          \n",
    "                                else:                                                                    \n",
    "                                    sublist['tokens'].insert(sublist['tokens'].index(element)+1, {'type': 'INS', 'value': max(bi_gram_dict[element['value'].lower()])[1]})\n",
    "                                    element['type'] = 'REF'    \n",
    "                                    inserted_words.append({'type': 'word', 'value': max(bi_gram_dict[element['value'].lower()])[1]})                                  \n",
    "                            else:                             \n",
    "                                subst_w = random.choice(one_gram_list)[1]                             #if the word not in bi-gram use insert a random word from uni-gram\n",
    "                                sublist['tokens'].insert(sublist['tokens'].index(element)+1, {'type': 'RND', 'value': subst_w})\n",
    "                                element['type'] = 'REF'\n",
    "                                inserted_words.append({'type': 'word', 'value': subst_w})                              \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for sublist in transcript: \n",
    "        for element in sublist['tokens']:\n",
    "            if element['type'] in('REF', 'INS', 'INS_SEC', 'RND'):\n",
    "                element['type'] = 'word'\n",
    "    return json.dumps(transcript), inserted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ASRforAD.csv')\n",
    "\n",
    "#insert % of words into json manual transcript for all rows in dataframe\n",
    "df = df.merge(df.json_utterances_man.apply(lambda s: pd.Series(insert_words(s, 0.2))), left_index=True, right_index=True)       \n",
    "df.rename(columns = {0:'json_utterances_man_with_INSERTED_WORDS_20%', 1:'INSERTED_WORDS_20%'}, inplace =True )         \n",
    "\n",
    "#output csv with altered manual transcript and inserted words as new columns \n",
    "df.to_csv('../INSERTION_ASRforAD.csv')\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
