{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from shared import *            #shared functions from shared.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemic Similarity\n",
    "From here: [https://stackoverflow.com/questions/26474847/estimate-phonemic-similarity-between-two-words] \n",
    "\n",
    "Web browser view: [http://www.greenteapress.com/thinkpython/code/c06d?fbclid=IwAR3kK8u0l48ksaGi8v60FZLDsSjpdjhw3dCCeZdRDS0VkBhgeR5YyzSUTuI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cmu_sound_dict():\n",
    "    \"\"\"create a phonemic dictionary\"\"\"\n",
    "    \n",
    "    cmu_final_sound_dict = {}\n",
    "    with open('../c06d') as cmu_dict:\n",
    "        cmu_dict = cmu_dict.read().split(\"\\n\")\n",
    "        for i in cmu_dict:\n",
    "            i_s = i.split()\n",
    "            if len(i_s) > 1:\n",
    "                word = i_s[0]\n",
    "                syllables = i_s[1:]\n",
    "            cmu_final_sound_dict[word.lower()] = \" \".join(syllables)\n",
    "    return cmu_final_sound_dict\n",
    "\n",
    "phonemic_model = create_cmu_sound_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcript Dictionary\n",
    "Create a dictionary for all words in transcript that contains a corresponding word with smallest Levenstein distance as well as distance itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tr_unigrm():\n",
    "    \"\"\"read in words from all the transcripts into a unique unigram list\"\"\"\n",
    "    \n",
    "    tr_unigrm_unique = []\n",
    "    with open('../tr_unigrams.txt', encoding='utf-8-sig') as tr_unigrm:\n",
    "        tr_unigrm = tr_unigrm.read().replace('\"','').replace('\\\\\\\\n', '').replace(\"\\\\\\\\t\", \"\")\n",
    "        tr_unigrm = tr_unigrm.split(\", \")\n",
    "        for i in range(len(tr_unigrm)):\n",
    "            if tr_unigrm[i] not in tr_unigrm_unique:\n",
    "                tr_unigrm_unique.append(tr_unigrm[i])\n",
    "    return tr_unigrm_unique\n",
    "\n",
    "tr_unigrm = create_tr_unigrm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tr_unigram_dict():\n",
    "    \"\"\"create a dictionary for all the unique unigrams from the tr_unigram list. \n",
    "    Dictionary will contain words as keys and values will have a list of the following elements: \n",
    "    a corresponding word that has a minimum Levenstein distance as well as the distance itself\"\"\"\n",
    "    \n",
    "    tr_unigrm_dict = {}\n",
    "    for j in range(len(tr_unigrm)):\n",
    "        if tr_unigrm[j] not in phonemic_model:  #if  unigram from transcript dictionary not in phonemic model return a random value from the reduced unigram list in shared file\n",
    "            tr_unigrm_dict[tr_unigrm[j]] = [random.choice(reduced_one_gram)[1], -1]   #set distance to -1      \n",
    "        else:                \n",
    "            temp_sub = []\n",
    "            temp_dist = []\n",
    "            for i in range(len(reduced_one_gram)):\n",
    "                if tr_unigrm[j] != reduced_one_gram[i][1]:\n",
    "                    if reduced_one_gram[i][1] in phonemic_model: \n",
    "                        temp_dist.append(nltk.edit_distance(phonemic_model[tr_unigrm[j]], phonemic_model[reduced_one_gram[i][1]], transpositions = False))\n",
    "                        temp_sub.append(reduced_one_gram[i][1])\n",
    "            tr_unigrm_dict[tr_unigrm[j]] = [temp_sub[temp_dist.index(min(temp_dist))], min(temp_dist)]\n",
    "    return tr_unigrm_dict\n",
    "\n",
    "tr_unigrm_dict = create_tr_unigram_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitution Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subst_words(transcript, rate):\n",
    "    \"\"\"substitute words in transcript at a given rate.\n",
    "    Return the new transcript and list of substituted words\"\"\"\n",
    "    \n",
    "    transcript = json.loads(transcript)\n",
    "    words_to_sub = random_words_list(flatten(transcript), rate)\n",
    "    substituted_words =[]\n",
    "    try:\n",
    "        while 0 != (len(words_to_sub)):\n",
    "            for sublist in transcript:                 \n",
    "                for element in sublist['tokens']:\n",
    "                    if words_to_sub[0] == element['value']: \n",
    "                        substituted_words.append(element)\n",
    "                        if words_to_sub[0] in tr_unigrm_dict.keys():\n",
    "                            element['value'] = tr_unigrm_dict[words_to_sub[0]['value']][0]\n",
    "                        else:\n",
    "                            element['value'] = random.choice(one_gram_list)[1]  # substitute with a random word from unigram\n",
    "                        words_to_sub.remove(words_to_sub[0])\n",
    "    except:\n",
    "        pass\n",
    "    return json.dumps(transcript), substituted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../ASRforAD.csv')\n",
    "\n",
    "#substitute % of words from json manual transcript with words that have shortest Levenstein distance\n",
    "df = df.merge(df.json_utterances_man.apply(lambda s: pd.Series(subst_words(s, 0.2))), left_index=True, right_index=True)\n",
    "df.rename(columns = {0:'json_utterances_man_with_SUBSTITUTED_WORDS_20%', 1:'SUBSTITUTED_WORDS_20%'}, inplace =True )\n",
    "\n",
    "#output csv with altered manual transcript and list of words that were substituted as new columns\n",
    "df.to_csv('../SUBSTITUTION_ASRforAD.csv')\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
